{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string(\"data_path\", None, \"Path to the dataset.\")\n",
    "flags.DEFINE_string(\n",
    "    \"dataset_name\", \"miniImageNet\", \"Name of the dataset to \"\n",
    "    \"train on, which will be mapped to data.MetaDataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --dataset_name before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9ced45099ccc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;31m# get too much noise.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnparsedFlagAccessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --dataset_name before flags were parsed."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import enum\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDIM = 640\n",
    "\n",
    "#name_tuple 생성\n",
    "ProblemInstance = collections.namedtuple(\n",
    "    \"ProblemInstance\",\n",
    "    [\"tr_input\", \"tr_output\", \"tr_info\", \"val_input\", \"val_output\", \"val_info\"])\n",
    "\n",
    "class StrEnum(enum.Enum):\n",
    "    \"\"\"An Enum represented by a string.\"\"\"\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class MetaDataset(StrEnum):\n",
    "    \"\"\"Datasets supported by the DataProvider class.\"\"\"\n",
    "    MINI = \"miniImageNet\"\n",
    "    TIERED = \"tieredImageNet\"\n",
    "\n",
    "\n",
    "# 이 친구 용도가 뭐냐....\n",
    "class EmbeddingCrop(StrEnum):\n",
    "    \"\"\"Embedding types supported by the DataProvider class.\"\"\"\n",
    "    CENTER = \"center\"\n",
    "    MULTIVIEW = \"multiview\"\n",
    "\n",
    "\n",
    "class MetaSplit(StrEnum):\n",
    "    \"\"\"Meta-datasets split supported by the DataProvider class.\"\"\"\n",
    "    TRAIN = \"train\"\n",
    "    VALID = \"val\"\n",
    "    TEST = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'dataset_split' and 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7d8211fdcbe4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataProvider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'dataset_split' and 'config'"
     ]
    }
   ],
   "source": [
    "class DataProvider(object):\n",
    "    \"\"\"Creates problem instances from a specific split and dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_split, config, verbose=False):\n",
    "        self._dataset_split = MetaSplit(dataset_split)\n",
    "        self._config = config\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self._check_config()\n",
    "\n",
    "        self._index_data(self._load_data())\n",
    "        \n",
    "    def _check_config(self):\n",
    "        \"\"\"Checks configuration arguments of constructor.\"\"\"\n",
    "        self._config[\"dataset_name\"] = MetaDataset(self._config[\"dataset_name\"])\n",
    "        # self._config[\"dataset_name\"] = \"miniImageNet\"\n",
    "\n",
    "        self._config[\"embedding_crop\"] = EmbeddingCrop(self._config[\"embedding_crop\"])\n",
    "        # self._config[\"embedding_crop\"] = \"center\"\n",
    "        \n",
    "        if self._config[\"dataset_name\"] == MetaDataset.TIERED:\n",
    "            error_message = \"embedding_crop: {} not supported for {}\".format(\n",
    "                self._config[\"embedding_crop\"], self._config[\"dataset_name\"])\n",
    "            assert self._config[\"embedding_crop\"] == EmbeddingCrop.CENTER, error_message\n",
    "            \n",
    "    def _load_data(self):\n",
    "        \"\"\"Loads data into memory and caches .\"\"\"\n",
    "        raw_data = self._load(tf.gfile.Open(self._get_full_pickle_path(self._dataset_split), \"rb\"))\n",
    "        if self._dataset_split == MetaSplit.TRAIN and self._config[\"train_on_val\"]:\n",
    "            valid_data = self._load(tf.gfile.Open(self._get_full_pickle_path(MetaSplit.VALID), \"rb\"))\n",
    "            for key in valid_data:\n",
    "                if self._verbose:\n",
    "                    tf.logging.info(str([key, raw_data[key].shape]))\n",
    "                raw_data[key] = np.concatenate([raw_data[key],valid_data[key]], axis=0)\n",
    "                if self._verbose:\n",
    "                    tf.logging.info(str([key, raw_data[key].shape]))\n",
    "\n",
    "        if self._verbose:\n",
    "            tf.logging.info(str([(k, np.shape(v)) for k, v in six.iteritems(raw_data)]))\n",
    "        return raw_data\n",
    "    \n",
    "    def _load(self, opened_file):\n",
    "        if six.PY2:\n",
    "            result = pickle.load(opened_file)\n",
    "        else:\n",
    "            result = pickle.load(opened_file, encoding=\"latin1\")  # pylint: disable=unexpected-keyword-arg\n",
    "        return result\n",
    "    \n",
    "a = DataProvider()\n",
    "a._load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
